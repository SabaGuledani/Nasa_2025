{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "233f4d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "from tqdm import tqdm\n",
    "from pymilvus import connections,Collection, utility, FieldSchema, CollectionSchema, DataType\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# for big model BAAI/bge-m3\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\", device=device)\n",
    "BATCH_SIZE = 100\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b201e094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6797221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bb48f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Milvus: v2.3.21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "connections.connect(\n",
    "    alias=\"default\",\n",
    "    host=\"127.0.0.1\",\n",
    "    port=\"19530\",\n",
    "    timeout=5,\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "print(\"Connected to Milvus:\", utility.get_server_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70582f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'publications' deleted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# incase I need to delete it\n",
    "collection_name = \"publications\"\n",
    "\n",
    "# Check if collection exists\n",
    "if utility.has_collection(collection_name):\n",
    "    utility.drop_collection(collection_name)\n",
    "    print(f\"Collection '{collection_name}' deleted.\")\n",
    "else:\n",
    "    print(f\"Collection '{collection_name}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "565215c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(folder_path:str) -> list:\n",
    "    \"\"\"load json files from specified folder into list, not load if text field is empty\"\"\"\n",
    "    docs = list()\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(os.path.join(folder_path, file) , \"r\", encoding=\"utf-8\") as doc:\n",
    "                data = json.load(doc)\n",
    "                if data[\"text\"] != \"\":\n",
    "                    docs.append(data)\n",
    "                data[\"PMC_code\"] = file.replace(\".json\", \"\").strip()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bc8fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"./data/publications_raw/\"\n",
    "documents = load_docs(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d060ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pub_name(publication:json):\n",
    "    \"\"\"clean publication name from trailing escape characters and spaces\"\"\"\n",
    "    publication[\"name\"] = publication[\"name\"].replace(r\"\\n\",\"\").strip()\n",
    "def clean_text(publication:json):\n",
    "    \"\"\"strip main text\"\"\"\n",
    "    publication[\"text\"] = publication[\"text\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e76649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    clean_pub_name(doc)\n",
    "    clean_text(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef15cfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Docs: 100%|██████████| 559/559 [00:03<00:00, 176.93it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=120)\n",
    "all_chunks = []\n",
    "\n",
    "for doc in tqdm(documents,\"Processing Docs\"):\n",
    "    chunks = text_splitter.create_documents([doc[\"text\"]])\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append({\n",
    "            \"PMC_code\": doc[\"PMC_code\"],\n",
    "            \"name\": doc[\"name\"],\n",
    "            \"text\" : chunk.page_content,\n",
    "            \"authors\":doc[\"authors\"],\n",
    "            \"date\":doc[\"date\"],\n",
    "            \"doi\": doc[\"doi\"]\n",
    "        })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8803f891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50552"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "969ca820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PMC_code': 'PMC10020673',\n",
       " 'name': 'Microbial isolation and characterization from two flex lines from the urine processor assembly onboard the international space station',\n",
       " 'text': 'Urine, humidity condensate, and other sources of non-potable water are processed onboard the International Space Station (ISS) by the Water Recovery System (WRS) yielding potable water. While some means of microbial control are in place, including a phosphoric acid/hexavalent chromium urine pretreatment solution, many areas within the WRS are not available for routine microbial monitoring. Due to refurbishment needs, two flex lines from the Urine Processor Assembly (UPA) within the WRS were removed and returned to Earth. The water from within these lines, as well as flush water, was',\n",
       " 'authors': ['Brian Crucian',\n",
       "  'Yo-Ann Velez Justiniano',\n",
       "  'Hang Ngoc Nguyen',\n",
       "  'Aubrie O’Rourke',\n",
       "  'Chelsea McCool',\n",
       "  'Sarah L Castro-Wallace',\n",
       "  'Miten Jain',\n",
       "  'Christian L Castro',\n",
       "  'Michael D Lee',\n",
       "  'Sarah Stahl-Rommel',\n",
       "  'Jill Williamson',\n",
       "  'Mayra Nelman-Gonzalez',\n",
       "  'Kenneth W Clark',\n",
       "  'G Marie Sharp'],\n",
       " 'date': '2023 Mar 2',\n",
       " 'doi': '10.1016/j.bioflm.2023.100108'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "872dda53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped_chunks:248\n"
     ]
    }
   ],
   "source": [
    "all_chunks_length_cleaned = []\n",
    "dropped_chunks = 0\n",
    "for chunk in all_chunks:\n",
    "    if \"List of\" not in chunk['text']:\n",
    "        if len(chunk['text'].strip()) > 100:\n",
    "            all_chunks_length_cleaned.append(chunk)\n",
    "            \n",
    "        else:\n",
    "            dropped_chunks +=1\n",
    "    else:\n",
    "        dropped_chunks+=1\n",
    "print(\"dropped_chunks:\" + str(dropped_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89d7ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def save_to_pickle(filename,object_to_save):\n",
    "    if \".pkl\" not in filename:\n",
    "        filename = filename+\".pkl\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(object_to_save,f)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    if \".pkl\" not in filename:\n",
    "        filename = filename + \".pkl\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "# save_to_pickle(\"semantic_chunks.pkl\", all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "448b1823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50552\n"
     ]
    }
   ],
   "source": [
    "print(len(all_chunks))\n",
    "all_chunks = all_chunks_length_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ab9ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "BATCH_SIZE = 256\n",
    "all_chunks = all_chunks_length_cleaned\n",
    "\n",
    "def embed_chunks_in_batches(chunks, batch_size=BATCH_SIZE):\n",
    "    vectors = []\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    batch_vectors = model.encode(\n",
    "        texts, \n",
    "        normalize_embeddings=True, \n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True\n",
    "    ) \n",
    "    vectors.extend(batch_vectors)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c18264f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9112f66be42f42d5bce4ea5d83429987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectors = embed_chunks_in_batches(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d546f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_pickle(\"vectors_publications_v1.pkl\", vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bddbae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = load_pickle(\"vectors_publications_v1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9520c5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "text_lengths = [len(c[\"text\"]) for c in all_chunks]\n",
    "max_text_length = max(text_lengths)\n",
    "print(max_text_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e16c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_chunks = [c for c in all_chunks if len(c[\"text\"]) > 600]\n",
    "for bc in bad_chunks[:5]:\n",
    "    print(len(bc[\"text\"]), repr(bc[\"text\"][:120]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02fd5b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(\n",
    "        name=\"id\", \n",
    "        dtype=DataType.INT64, \n",
    "        is_primary=True, \n",
    "        auto_id=True\n",
    "    ),\n",
    "    FieldSchema(\n",
    "        name=\"embedding\",\n",
    "        dtype=DataType.FLOAT_VECTOR,\n",
    "        dim=384, \n",
    "        metric_type=\"COSINE\"\n",
    "    ),\n",
    "    FieldSchema(\n",
    "        name=\"PMC_code\",\n",
    "        dtype=DataType.VARCHAR,\n",
    "        max_length=20\n",
    "    ),\n",
    "    FieldSchema(\n",
    "        name=\"name\",\n",
    "        dtype=DataType.VARCHAR,\n",
    "        max_length=300\n",
    "    ),\n",
    "    FieldSchema(\n",
    "        name=\"content\",\n",
    "        dtype=DataType.VARCHAR,\n",
    "        max_length=2000\n",
    "    ),\n",
    "    FieldSchema(\n",
    "        name=\"authors\", \n",
    "        dtype=DataType.VARCHAR, \n",
    "        max_length=2000  # store as comma-separated or JSON\n",
    "    ),\n",
    "    FieldSchema(\n",
    "        name=\"date\",\n",
    "        dtype=DataType.VARCHAR,\n",
    "        max_length=20  # e.g. \"2024-09-30\"\n",
    "    ),\n",
    "    FieldSchema(\n",
    "        name=\"doi\",\n",
    "        dtype=DataType.VARCHAR,\n",
    "        max_length=200\n",
    "    ),\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields, description=\"RAG collection with publication metadata\")\n",
    "collection = Collection(\"publications\", schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e65a8b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "for i in range(0, len(all_chunks), BATCH_SIZE):\n",
    "    batch_vectors = vectors[i:i+BATCH_SIZE]\n",
    "    batch_pmc_codes = [c[\"PMC_code\"] for c in all_chunks[i:i+BATCH_SIZE]]\n",
    "    batch_names = [c[\"name\"] for c in all_chunks[i:i+BATCH_SIZE]]\n",
    "    batch_texts = [c[\"text\"] for c in all_chunks[i:i+BATCH_SIZE]]\n",
    "    batch_authors = [\",\".join(c[\"authors\"]) for c in all_chunks[i:i+BATCH_SIZE]]\n",
    "    batch_dates = []\n",
    "    batch_doi = []\n",
    "    for c in all_chunks[i:i+BATCH_SIZE]:\n",
    "        date = c[\"date\"]\n",
    "        if date:\n",
    "            dt = datetime.strptime(date, \"%Y %b %d\")\n",
    "            formatted = dt.strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "            formatted = \"None\"\n",
    "        \n",
    "        batch_dates.append(formatted)\n",
    "\n",
    "        doi = c[\"doi\"]\n",
    "        if doi:\n",
    "            batch_doi.append(doi)\n",
    "        else:\n",
    "            doi = \"None\"\n",
    "            batch_doi.append(doi)\n",
    "        \n",
    "        \n",
    "\n",
    "    collection.insert([batch_vectors, batch_pmc_codes, batch_names, batch_texts,batch_authors,batch_dates,batch_doi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815fc83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
